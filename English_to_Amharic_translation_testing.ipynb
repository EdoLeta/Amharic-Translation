{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "English_to_Amharic_translation_testing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdoLeta/Amharic-Translation/blob/main/English_to_Amharic_translation_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ5S-hp6Ez2f"
      },
      "source": [
        "*English to Amharic Translation using Neural Machine Translation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UsLDHm-Jw5P",
        "outputId": "1a68f66f-a6fb-466d-c7de-c3b2339ac103"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT6gOLJ1apgr"
      },
      "source": [
        "# TODO: Set your source and target languages. Keep in mind, these traditionally use language codes as found here:\n",
        "# These will also become the suffix's of all vocab and corpus files used throughout\n",
        "import os\n",
        "source_language = \"en\"\n",
        "target_language = \"am\"\n",
        "tag = \"baseline\" # Give a unique name to your folder - this is to ensure you don't rewrite any models you've already submitted\n",
        "\n",
        "os.environ[\"src\"] = source_language # Sets them in bash as well, since we often use bash scripts\n",
        "os.environ[\"tgt\"] = target_language\n",
        "os.environ[\"tag\"] = tag\n",
        "\n",
        "# This will save it to a folder in our gdrive instead!\n",
        "!mkdir -p \"/content/drive/MyDrive/Colab Notebooks/en-am translation/$src-$tgt\"\n",
        "os.environ[\"gdrive_path\"] = \"/content/drive/MyDrive/Colab Notebooks/en-am translation/%s-%s\" % (source_language, target_language)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "RhI5rCxkLOQz",
        "outputId": "c48b38bc-b972-45ab-bb86-ede506e3394f"
      },
      "source": [
        "# I am training the data with another dataset that I created have created for this specific domain\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-247a89a9-5ef0-45be-9765-ec7f5a6c3c7e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-247a89a9-5ef0-45be-9765-ec7f5a6c3c7e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving en_am.csv to en_am.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TsgIUEavc9K"
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "data = pd.read_csv(io.BytesIO(uploaded['en_am.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "66qYqydYwaSB",
        "outputId": "faaff1e1-9818-4d86-e8b0-f9964b678156"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Amharic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17  When Aʹbram was 99 years old, Jehovah appe...</td>\n",
              "      <td>17  አብራም ዕድሜው 99 ዓመት ሲሆን ይሖዋ ተገለጠለትና እንዲህ አለው፦...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2  I will establish my covenant between me and...</td>\n",
              "      <td>2  ከአንተ ጋር ቃል ኪዳኔን እመሠርታለሁ፤+ አንተንም እጅግ አበዛሃለሁ።”+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3  At this Aʹbram fell facedown, and God conti...</td>\n",
              "      <td>3  በዚህ ጊዜ አብራም በግንባሩ ተደፋ፤ አምላክም እንዲህ በማለት እሱን ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4  “As for me, look! my covenant is with you,+...</td>\n",
              "      <td>4  “በእኔ በኩል ከአንተ ጋር የገባሁት ቃል ኪዳኔ እንደጸና ነው፤+ አን...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5  Your name will no longer be Aʹbram;* your n...</td>\n",
              "      <td>5  ከእንግዲህ ስምህ አብራም* አይባልም፤ የብዙ ብሔር አባት ስለማደርግህ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                             English                                            Amharic\n",
              "0  17  When Aʹbram was 99 years old, Jehovah appe...  17  አብራም ዕድሜው 99 ዓመት ሲሆን ይሖዋ ተገለጠለትና እንዲህ አለው፦...\n",
              "1  2  I will establish my covenant between me and...  2  ከአንተ ጋር ቃል ኪዳኔን እመሠርታለሁ፤+ አንተንም እጅግ አበዛሃለሁ።”+ \n",
              "2  3  At this Aʹbram fell facedown, and God conti...  3  በዚህ ጊዜ አብራም በግንባሩ ተደፋ፤ አምላክም እንዲህ በማለት እሱን ...\n",
              "3  4  “As for me, look! my covenant is with you,+...  4  “በእኔ በኩል ከአንተ ጋር የገባሁት ቃል ኪዳኔ እንደጸና ነው፤+ አን...\n",
              "4  5  Your name will no longer be Aʹbram;* your n...  5  ከእንግዲህ ስምህ አብራም* አይባልም፤ የብዙ ብሔር አባት ስለማደርግህ..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 787
        },
        "id": "XIktrj1xNVC-",
        "outputId": "8a173281-738c-42de-c9a4-5b2c3a1bcfc0"
      },
      "source": [
        "data[data.duplicated()]"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>English</th>\n",
              "      <th>Amharic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3597</th>\n",
              "      <td>26  Jehovah spoke further to Moses, saying:</td>\n",
              "      <td>26  በተጨማሪም ይሖዋ ሙሴን እንዲህ አለው፦</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4827</th>\n",
              "      <td>17  Jehovah went on to say to Moses:</td>\n",
              "      <td>17  ይሖዋ ሙሴን እንዲህ አለው፦</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10198</th>\n",
              "      <td>4  However, the high places were not removed,+...</td>\n",
              "      <td>4  ሆኖም ከፍ ያሉት የማምለኪያ ቦታዎች አልተወገዱም ነበር፤+ ሕዝቡ አሁ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11859</th>\n",
              "      <td>4  “Your father made our yoke harsh.+ But if y...</td>\n",
              "      <td>4  “አባትህ ቀንበራችንን አክብዶብን ነበር።+ አንተ ግን አባትህ የሰጠን...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11861</th>\n",
              "      <td>6  King Re·ho·boʹam then consulted with the ol...</td>\n",
              "      <td>6  ከዚያም ንጉሥ ሮብዓም አባቱ ሰለሞን በሕይወት በነበረበት ጊዜ ሲያገለ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11863</th>\n",
              "      <td>8  However, he rejected the advice that the ol...</td>\n",
              "      <td>8  ሆኖም ሮብዓም ሽማግሌዎቹ የሰጡትን ምክር ትቶ አብሮ አደጎቹ ከነበሩት...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11864</th>\n",
              "      <td>9  He asked them: “What advice do you offer on...</td>\n",
              "      <td>9  እሱም እንዲህ ሲል ጠየቃቸው፦ “‘አባትህ የጫነብንን ቀንበር አቅልልል...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11867</th>\n",
              "      <td>12  Jer·o·boʹam and all the people came to Re·...</td>\n",
              "      <td>12  ኢዮርብዓምና መላው ሕዝብ ንጉሡ “በሦስተኛው ቀን ተመልሳችሁ ኑ” ባ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11872</th>\n",
              "      <td>17  But Re·ho·boʹam continued to reign over th...</td>\n",
              "      <td>17  ይሁንና ሮብዓም በይሁዳ ከተሞች በሚኖሩት እስራኤላውያን ላይ መግዛቱ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16352</th>\n",
              "      <td>5  Be exalted above the heavens, O God;May yo...</td>\n",
              "      <td>5  አምላክ ሆይ፣ ከሰማያት በላይ ከፍ ከፍ በል፤ክብርህ በምድር ሁሉ ላ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18359</th>\n",
              "      <td>3  They said to him: “This is what Hez·e·kiʹah...</td>\n",
              "      <td>3  እነሱም እንዲህ አሉት፦ “ሕዝቅያስ እንዲህ ይላል፦ ‘ይህ ቀን የጭንቀ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18361</th>\n",
              "      <td>5  So the servants of King Hez·e·kiʹah went in...</td>\n",
              "      <td>5  በመሆኑም የንጉሥ ሕዝቅያስ አገልጋዮች ወደ ኢሳይያስ ሄዱ፤+</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18366</th>\n",
              "      <td>10  “This is what you should say to King Hez·e...</td>\n",
              "      <td>10  “የይሁዳን ንጉሥ ሕዝቅያስን እንዲህ በሉት፦ ‘የምትታመንበት አምላክ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18367</th>\n",
              "      <td>11  Look! You have heard what the kings of As·...</td>\n",
              "      <td>11  እነሆ፣ የአሦር ነገሥታት ሌሎቹን አገሮች ሁሉ ፈጽመው በማጥፋት ያደ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18370</th>\n",
              "      <td>14  Hez·e·kiʹah took the letters out of the ha...</td>\n",
              "      <td>14  ሕዝቅያስ ደብዳቤዎቹን ከመልእክተኞቹ እጅ ተቀብሎ አነበበ። ከዚያም ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21021</th>\n",
              "      <td>11  And the word of Jehovah again came to me, ...</td>\n",
              "      <td>11  የይሖዋም ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21309</th>\n",
              "      <td>16  The word of Jehovah again came to me, saying:</td>\n",
              "      <td>16  የይሖዋ ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21459</th>\n",
              "      <td>2  Then the word of Jehovah came to me, saying:</td>\n",
              "      <td>2  ከዚያም የይሖዋ ቃል እንዲህ ሲል ወደ እኔ መጣ፦</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21635</th>\n",
              "      <td>18  And the word of Jehovah again came to me, ...</td>\n",
              "      <td>18  የይሖዋም ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21650</th>\n",
              "      <td>15  The word of Jehovah again came to me, saying:</td>\n",
              "      <td>15  የይሖዋ ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21674</th>\n",
              "      <td>17  And the word of Jehovah again came to me, ...</td>\n",
              "      <td>17  የይሖዋም ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24480</th>\n",
              "      <td>5  Others fell on rocky ground where there was...</td>\n",
              "      <td>5  ሌሎቹ ደግሞ ብዙ አፈር በሌለው ድንጋያማ መሬት ላይ ወደቁ፤ አፈሩም ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24481</th>\n",
              "      <td>6  But when the sun rose, they were scorched, ...</td>\n",
              "      <td>6  ፀሐይ በወጣ ጊዜ ግን ተቃጠሉ፤ ሥር ስላልነበራቸውም ደረቁ።</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29205</th>\n",
              "      <td>2  May you have undeserved kindness and peace ...</td>\n",
              "      <td>2  አባታችን ከሆነው አምላክና ከጌታ ኢየሱስ ክርስቶስ ጸጋና ሰላም ለእና...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29360</th>\n",
              "      <td>2  May you have undeserved kindness and peace ...</td>\n",
              "      <td>2  አባታችን ከሆነው አምላክና ከጌታ ኢየሱስ ክርስቶስ ጸጋና ሰላም ለእና...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 English                                            Amharic\n",
              "3597         26  Jehovah spoke further to Moses, saying:                       26  በተጨማሪም ይሖዋ ሙሴን እንዲህ አለው፦\n",
              "4827                17  Jehovah went on to say to Moses:                              17  ይሖዋ ሙሴን እንዲህ አለው፦\n",
              "10198  4  However, the high places were not removed,+...  4  ሆኖም ከፍ ያሉት የማምለኪያ ቦታዎች አልተወገዱም ነበር፤+ ሕዝቡ አሁ...\n",
              "11859  4  “Your father made our yoke harsh.+ But if y...  4  “አባትህ ቀንበራችንን አክብዶብን ነበር።+ አንተ ግን አባትህ የሰጠን...\n",
              "11861  6  King Re·ho·boʹam then consulted with the ol...  6  ከዚያም ንጉሥ ሮብዓም አባቱ ሰለሞን በሕይወት በነበረበት ጊዜ ሲያገለ...\n",
              "11863  8  However, he rejected the advice that the ol...  8  ሆኖም ሮብዓም ሽማግሌዎቹ የሰጡትን ምክር ትቶ አብሮ አደጎቹ ከነበሩት...\n",
              "11864  9  He asked them: “What advice do you offer on...  9  እሱም እንዲህ ሲል ጠየቃቸው፦ “‘አባትህ የጫነብንን ቀንበር አቅልልል...\n",
              "11867  12  Jer·o·boʹam and all the people came to Re·...  12  ኢዮርብዓምና መላው ሕዝብ ንጉሡ “በሦስተኛው ቀን ተመልሳችሁ ኑ” ባ...\n",
              "11872  17  But Re·ho·boʹam continued to reign over th...  17  ይሁንና ሮብዓም በይሁዳ ከተሞች በሚኖሩት እስራኤላውያን ላይ መግዛቱ...\n",
              "16352   5  Be exalted above the heavens, O God;May yo...   5  አምላክ ሆይ፣ ከሰማያት በላይ ከፍ ከፍ በል፤ክብርህ በምድር ሁሉ ላ...\n",
              "18359  3  They said to him: “This is what Hez·e·kiʹah...  3  እነሱም እንዲህ አሉት፦ “ሕዝቅያስ እንዲህ ይላል፦ ‘ይህ ቀን የጭንቀ...\n",
              "18361  5  So the servants of King Hez·e·kiʹah went in...           5  በመሆኑም የንጉሥ ሕዝቅያስ አገልጋዮች ወደ ኢሳይያስ ሄዱ፤+\n",
              "18366  10  “This is what you should say to King Hez·e...  10  “የይሁዳን ንጉሥ ሕዝቅያስን እንዲህ በሉት፦ ‘የምትታመንበት አምላክ...\n",
              "18367  11  Look! You have heard what the kings of As·...  11  እነሆ፣ የአሦር ነገሥታት ሌሎቹን አገሮች ሁሉ ፈጽመው በማጥፋት ያደ...\n",
              "18370  14  Hez·e·kiʹah took the letters out of the ha...  14  ሕዝቅያስ ደብዳቤዎቹን ከመልእክተኞቹ እጅ ተቀብሎ አነበበ። ከዚያም ...\n",
              "21021  11  And the word of Jehovah again came to me, ...                11  የይሖዋም ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦\n",
              "21309  16  The word of Jehovah again came to me, saying:                 16  የይሖዋ ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦\n",
              "21459    2  Then the word of Jehovah came to me, saying:                  2  ከዚያም የይሖዋ ቃል እንዲህ ሲል ወደ እኔ መጣ፦\n",
              "21635  18  And the word of Jehovah again came to me, ...                18  የይሖዋም ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦\n",
              "21650  15  The word of Jehovah again came to me, saying:                 15  የይሖዋ ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦\n",
              "21674  17  And the word of Jehovah again came to me, ...                17  የይሖዋም ቃል ዳግመኛ እንዲህ ሲል ወደ እኔ መጣ፦\n",
              "24480  5  Others fell on rocky ground where there was...  5  ሌሎቹ ደግሞ ብዙ አፈር በሌለው ድንጋያማ መሬት ላይ ወደቁ፤ አፈሩም ...\n",
              "24481  6  But when the sun rose, they were scorched, ...           6  ፀሐይ በወጣ ጊዜ ግን ተቃጠሉ፤ ሥር ስላልነበራቸውም ደረቁ።\n",
              "29205  2  May you have undeserved kindness and peace ...  2  አባታችን ከሆነው አምላክና ከጌታ ኢየሱስ ክርስቶስ ጸጋና ሰላም ለእና...\n",
              "29360  2  May you have undeserved kindness and peace ...  2  አባታችን ከሆነው አምላክና ከጌታ ኢየሱስ ክርስቶስ ጸጋና ሰላም ለእና..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHtFaZtlwcYd"
      },
      "source": [
        "data = data.rename(columns={\"English\":\"source_sentence\", \"Amharic\":\"target_sentence\"})"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeILBPq8MXhs",
        "outputId": "c7d3d0c0-77b2-45ad-96d3-2fe9066f7c61"
      },
      "source": [
        "print(\"Length of Data before Removing duplicate: \",len(data))\n",
        "data = data.drop_duplicates()\n",
        "print(\"Length of Data after Removing duplicate: \",len(data))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Data before Removing duplicate:  31078\n",
            "Length of Data after Removing duplicate:  31053\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tyODzTVfx5GO"
      },
      "source": [
        "# Do the split between dev/test/train and create parallel corpora\n",
        "num_dev_patterns = 1000\n",
        "num_test_patterns = 1000\n",
        "df = data\n",
        "# Lower case the corpora\n",
        "df[\"source_sentence\"] = df[\"source_sentence\"].str.lower()\n",
        "df[\"target_sentence\"] = df[\"target_sentence\"].str.lower()\n",
        "\n",
        "\n",
        "devtest = df.tail(num_dev_patterns + num_test_patterns)\n",
        "test = devtest.tail(num_test_patterns)\n",
        "dev = devtest.head(num_dev_patterns)\n",
        "stripped = df.drop(df.tail(num_dev_patterns + num_test_patterns).index)\n",
        "\n",
        "stripped[[\"source_sentence\"]].to_csv(\"train.en\", index=False)\n",
        "stripped[[\"target_sentence\"]].to_csv(\"train.am\", index=False)\n",
        "\n",
        "dev[[\"source_sentence\"]].to_csv(\"dev.en\", index=False)\n",
        "dev[[\"target_sentence\"]].to_csv(\"dev.am\", index=False)\n",
        "\n",
        "test[[\"source_sentence\"]].to_csv(\"test.en\", index=False)\n",
        "test[[\"target_sentence\"]].to_csv(\"test.am\", index=False)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wb7MZ58MXitm",
        "outputId": "32ff2986-b210-4017-c962-fa01fbc7abe5"
      },
      "source": [
        "! git clone https://github.com/joeynmt/joeynmt.git\n",
        "! cd joeynmt; pip3 install ."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'joeynmt'...\n",
            "remote: Enumerating objects: 138, done.\u001b[K\n",
            "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
            "remote: Compressing objects: 100% (100/100), done.\u001b[K\n",
            "remote: Total 3089 (delta 77), reused 75 (delta 38), pack-reused 2951\u001b[K\n",
            "Receiving objects: 100% (3089/3089), 8.08 MiB | 7.49 MiB/s, done.\n",
            "Resolving deltas: 100% (2106/2106), done.\n",
            "Processing /content/joeynmt\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.16.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (7.1.2)\n",
            "Collecting numpy==1.20.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/8a/064b4077e3d793f877e3b77aa64f56fa49a4d37236a53f78ee28be009a16/numpy-1.20.1-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 213kB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (54.2.0)\n",
            "Collecting torch==1.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/99/5861239a6e1ffe66e120f114a4d67e96e5c4b17c1a785dfc6ca6769585fc/torch-1.8.0-cp37-cp37m-manylinux1_x86_64.whl (735.5MB)\n",
            "\u001b[K     |████████████████████████████████| 735.5MB 25kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=1.15 in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (2.4.1)\n",
            "Collecting torchtext==0.9.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/50/84184d6230686e230c464f0dd4ff32eada2756b4a0b9cefec68b88d1d580/torchtext-0.9.0-cp37-cp37m-manylinux1_x86_64.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 30.7MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.3.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[?25hCollecting subword-nmt\n",
            "  Downloading https://files.pythonhosted.org/packages/74/60/6600a7bc09e7ab38bc53a48a20d8cae49b837f93f5842a41fe513a694912/subword_nmt-0.3.7-py2.py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from joeynmt==1.3) (0.11.1)\n",
            "Collecting pyyaml>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 52.1MB/s \n",
            "\u001b[?25hCollecting pylint\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/93/c710ecf2dd7098e9e15a0760c3afa31ae0b586beb4184bf4b14341c36848/pylint-2.7.4-py3-none-any.whl (346kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 59.9MB/s \n",
            "\u001b[?25hCollecting six==1.12\n",
            "  Downloading https://files.pythonhosted.org/packages/73/fb/00a976f728d0d1fecfe898238ce23f502a721c0ac0ecfedb80e0d88c64e9/six-1.12.0-py2.py3-none-any.whl\n",
            "Collecting wrapt==1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0->joeynmt==1.3) (3.7.4.3)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.28.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.32.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.3.4)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (3.12.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (1.8.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.36.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=1.15->joeynmt==1.3) (0.4.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.9.0->joeynmt==1.3) (4.41.1)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->joeynmt==1.3) (2.8.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.1.5)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn->joeynmt==1.3) (1.4.1)\n",
            "Collecting isort<6,>=4.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d9/47/0ec3ec948b7b3a0ba44e62adede4dca8b5985ba6aaee59998bed0916bd17/isort-5.8.0-py3-none-any.whl (103kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 51.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from pylint->joeynmt==1.3) (0.10.2)\n",
            "Collecting astroid<2.7,>=2.5.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/e3/9f9c3ad230d706dbf2d0314f5cff92f6d3cc7d65226af2854ea59d000c43/astroid-2.5.3-py3-none-any.whl (226kB)\n",
            "\u001b[K     |████████████████████████████████| 235kB 56.7MB/s \n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/87/89/479dc97e18549e21354893e4ee4ef36db1d237534982482c3681ee6e7b57/mccabe-0.6.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.15->joeynmt==1.3) (1.24.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (1.3.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn->joeynmt==1.3) (2018.9)\n",
            "Collecting lazy-object-proxy>=1.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/b0/f055db25fd68ab4859832a887c8b304274fc12dd5a3f8e83e61250733aeb/lazy_object_proxy-1.6.0-cp37-cp37m-manylinux1_x86_64.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 8.8MB/s \n",
            "\u001b[?25hCollecting typed-ast<1.5,>=1.4.0; implementation_name == \"cpython\" and python_version < \"3.8\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/65/b3/573d2f1fecbbe8f82a8d08172e938c247f99abe1be3bef3da2efaa3810bf/typed_ast-1.4.3-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 51.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=1.15->joeynmt==1.3) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.15->joeynmt==1.3) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->joeynmt==1.3) (3.1.0)\n",
            "Building wheels for collected packages: joeynmt, wrapt\n",
            "  Building wheel for joeynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for joeynmt: filename=joeynmt-1.3-cp37-none-any.whl size=84842 sha256=30e89f89074119b535220112a9e141a7a78f703148dca5875011d7d2c651c346\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ceg3kp0f/wheels/db/01/db/751cc9f3e7f6faec127c43644ba250a3ea7ad200594aeda70a\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.11.1-cp37-cp37m-linux_x86_64.whl size=68393 sha256=e03889f8cbaf14328576dfeb49f1933c8ca2ff46b5b34d279c38534f006dd45a\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built joeynmt wrapt\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.8.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement wrapt~=1.12.1, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-python-client 1.12.8 has requirement six<2dev,>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-api-core 1.26.3 has requirement six>=1.13.0, but you'll have six 1.12.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, torch, torchtext, portalocker, sacrebleu, subword-nmt, pyyaml, isort, wrapt, lazy-object-proxy, typed-ast, astroid, mccabe, pylint, six, joeynmt\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "  Found existing installation: torch 1.8.1+cu101\n",
            "    Uninstalling torch-1.8.1+cu101:\n",
            "      Successfully uninstalled torch-1.8.1+cu101\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: wrapt 1.12.1\n",
            "    Uninstalling wrapt-1.12.1:\n",
            "      Successfully uninstalled wrapt-1.12.1\n",
            "  Found existing installation: six 1.15.0\n",
            "    Uninstalling six-1.15.0:\n",
            "      Successfully uninstalled six-1.15.0\n",
            "Successfully installed astroid-2.5.3 isort-5.8.0 joeynmt-1.3 lazy-object-proxy-1.6.0 mccabe-0.6.1 numpy-1.20.1 portalocker-2.0.0 pylint-2.7.4 pyyaml-5.4.1 sacrebleu-1.5.1 six-1.12.0 subword-nmt-0.3.7 torch-1.8.0 torchtext-0.9.0 typed-ast-1.4.3 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoLltS6xybX3",
        "outputId": "5f7a17b4-9fe4-462c-b487-cea26abe8037"
      },
      "source": [
        "\n",
        "# One of the huge boosts in NMT performance was to use a different method of tokenizing. \n",
        "# Usually, NMT would tokenize by words. However, using a method called BPE gave amazing boosts to performance\n",
        "\n",
        "# Do subword NMT\n",
        "! mkdir joeynmt/data/\n",
        "! mkdir joeynmt/data/enam/\n",
        "! export data_path=joeynmt/data/$src$tgt/\n",
        "! subword-nmt learn-joint-bpe-and-vocab --input train.$src train.$tgt -s 4000 -o bpe.codes.4000 --write-vocabulary vocab.$src vocab.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < train.$src > train.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < train.$tgt > train.bpe.$tgt\n",
        "\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < dev.$src > dev.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < dev.$tgt > dev.bpe.$tgt\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$src < test.$src > test.bpe.$src\n",
        "! subword-nmt apply-bpe -c bpe.codes.4000 --vocabulary vocab.$tgt < test.$tgt > test.bpe.$tgt\n",
        "\n",
        "# Create directory, move everyone we care about to the correct location\n",
        "#! mkdir -p $data_path\n",
        "! cp train.* joeynmt/data/enam/\n",
        "! cp test.* joeynmt/data/enam/\n",
        "! cp dev.* joeynmt/data/enam/\n",
        "! cp bpe.codes.4000 $data_path\n",
        "! ls $data_path\n",
        "\n",
        "# Create that vocab using build_vocab\n",
        "! sudo chmod 777 joeynmt/scripts/build_vocab.py\n",
        "! joeynmt/scripts/build_vocab.py joeynmt/data/$src$tgt/train.bpe.$src joeynmt/data/$src$tgt/train.bpe.$tgt --output_path joeynmt/data/$src$tgt/vocab.txt\n",
        "\n",
        "# Some output\n",
        "! echo \"BPE amharic Sentences\"\n",
        "! tail -n 5 test.bpe.$tgt\n",
        "! echo \"Combined BPE Vocab\"\n",
        "! tail -n 10 joeynmt/data/enam/vocab.txt"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘joeynmt/data/’: File exists\n",
            "mkdir: cannot create directory ‘joeynmt/data/enam/’: File exists\n",
            "cp: missing destination file operand after 'bpe.codes.4000'\n",
            "Try 'cp --help' for more information.\n",
            "bpe.codes.4000\tdev.en\t\tjoeynmt      test.bpe.en   train.bpe.en\n",
            "dev.am\t\tdrive\t\tsample_data  test.en\t   train.en\n",
            "dev.bpe.am\ten_am.csv\ttest.am      train.am\t   vocab.am\n",
            "dev.bpe.en\ten_am_data.csv\ttest.bpe.am  train.bpe.am  vocab.en\n",
            "BPE amharic Sentences\n",
            "17 መንፈ@@ ሱና ሙ@@ ሽ@@ ራ@@ ይ@@ ቱ@@ ም+ “@@ ና@@ !” ይላ@@ ሉ፤ የሚ@@ ሰማ@@ ም ሁሉ “@@ ና@@ !” ይ@@ በ@@ ል፤ የተ@@ ጠ@@ ማ@@ ም ሁሉ ይ@@ ምጣ@@ ፤+ የሚ@@ ፈል@@ ግ ሁሉ የ@@ ሕይወ@@ ትን ውኃ በ@@ ነፃ ይ@@ ውሰ@@ ድ@@ ።+ \n",
            "18 “@@ በዚህ ጥ@@ ቅል@@ ል ላይ የሰ@@ ፈ@@ ሩትን የ@@ ትንቢት ቃ@@ ላት ለሚ@@ ሰማ ሁሉ እ@@ መሠ@@ ክ@@ ራ@@ ለሁ@@ ፦ ማንም በ@@ እነዚህ ነገሮች ላይ አንዲት ነገር ቢ@@ ጨ@@ ምር@@ + አምላክ በዚህ ጥ@@ ቅል@@ ል ውስጥ የተ@@ ጻ@@ ፉ@@ ትን መቅ@@ ሰ@@ ፍ@@ ቶች ይ@@ ጨ@@ ምር@@ በታ@@ ል፤+\n",
            "19 ማንም በዚህ የ@@ ትንቢት መጽሐ@@ ፍ ጥ@@ ቅል@@ ል ላይ ከሰ@@ ፈ@@ ሩት ቃ@@ ላት አንዲት ነገር ቢያ@@ ጎ@@ ድል አምላክ በዚህ ጥ@@ ቅል@@ ል ላይ ከተ@@ ጠ@@ ቀ@@ ሱ@@ ት የ@@ ሕይወት ዛ@@ ፎ@@ ች@@ ና+ ከ@@ ቅ@@ ድ@@ ስ@@ ቲ@@ ቱ ከተማ@@ + ዕ@@ ጣ ፋ@@ ን@@ ታ@@ ውን ይወ@@ ስድ@@ በታ@@ ል። \n",
            "20 “@@ ስለ እነዚህ ነገሮች የሚ@@ መሠ@@ ክ@@ ረው ‘@@ አዎ፣ ቶ@@ ሎ እ@@ መጣ@@ ለሁ@@ ’+ ይላ@@ ል።” “@@ አ@@ ሜ@@ ን@@ ! ጌታ ኢየሱ@@ ስ፣ ና@@ ።” \n",
            "21 የ@@ ጌታ የ@@ ኢየሱስ ጸ@@ ጋ ከ@@ ቅዱ@@ ሳ@@ ኑ ጋር ይሁን@@ ።\n",
            "Combined BPE Vocab\n",
            "ኋ\n",
            "isra@@\n",
            "betwe@@\n",
            "‘\n",
            "ቾ\n",
            "ኼ\n",
            "ח@@\n",
            "ז\n",
            "“\n",
            "ጱ\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X9xZAtq7KBy1",
        "outputId": "729e866a-1eb7-4e19-ec3b-ecdd90836dd5"
      },
      "source": [
        "# Also move everything we care about to a mounted location in google drive (relevant if running in colab) at gdrive_path\n",
        "! cp train.* \"$gdrive_path\"\n",
        "! cp test.* \"$gdrive_path\"\n",
        "! cp dev.* \"$gdrive_path\"\n",
        "! cp bpe.codes.4000 \"$gdrive_path\"\n",
        "! ls \"$gdrive_path\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bpe.codes.4000\tdev.bpe.en  test.am\t test.en       train.bpe.en\n",
            "dev.am\t\tdev.en\t    test.bpe.am  train.am      train.en\n",
            "dev.bpe.am\tmodels\t    test.bpe.en  train.bpe.am\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGt_qlpicmKe"
      },
      "source": [
        "def get_last_checkpoint(directory):\n",
        "  last_checkpoint = ''\n",
        "  for filename in os.listdir(directory):\n",
        "    if not 'best' in filename and filename.endswith(\".ckpt\"):\n",
        "        if not last_checkpoint or int(filename.split('.')[0]) > int(last_checkpoint.split('.')[0]):\n",
        "          last_checkpoint = filename\n",
        "  return last_checkpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzgcbnQhdjON",
        "outputId": "d4636522-f60e-4a49-a3dc-67c5a296ca87"
      },
      "source": [
        "g_drive_path = \"/content/drive/MyDrive/Colab Notebooks/en-am translation\"\n",
        "# g_drive_path = \"/content/drive/My Drive/masakhane/%s-%s-%s\" % (source_language, target_language, tag)\n",
        "print(g_drive_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/en-am translation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "weMuBVtedYkZ",
        "outputId": "1158c54f-f3a5-4266-ee39-ec963a2cdcf3"
      },
      "source": [
        "models_path = \"/content/drive/MyDrive/Colab Notebooks/en-am translation/models\"\n",
        "print(models_path)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/en-am translation/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jeOrYVTmbEs",
        "outputId": "e78890f7-104f-4c42-a74c-e7535df14a3b"
      },
      "source": [
        "# Copy the created models from the temporary storage to main storage on google drive for persistant storage \n",
        "# the content of te folder will be overwrite when you start trainin\n",
        "# !cp -r \"/content/drive/MyDrive/masakhane/en-am/models\"* \"$gdrive_path/models/${src}${tgt}_transformer/\"\n",
        "last_checkpoint = get_last_checkpoint(models_path)\n",
        "print('Last checkpoint :',last_checkpoint)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Last checkpoint : 330000.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBTUgtY2zCZO"
      },
      "source": [
        "# !mkdir -p /content/joeynmt/configs/models"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8qTiSVqm57O"
      },
      "source": [
        "\n",
        "# This creates the config file for our JoeyNMT system. It might seem overwhelming so we've provided a couple of useful parameters you'll need to update\n",
        "# (You can of course play with all the parameters if you'd like!)\n",
        "name = '%s%s' % (source_language, target_language)\n",
        "\n",
        "config = \"\"\"\n",
        "name: \"{name}_transformer\"\n",
        "\n",
        "data:\n",
        "    src: \"{source_language}\"\n",
        "    trg: \"{target_language}\"\n",
        "    train: \"data/{name}/train.bpe\"\n",
        "    dev:   \"data/{name}/dev.bpe\"\n",
        "    test:  \"data/{name}/test.bpe\"\n",
        "    level: \"bpe\"\n",
        "    lowercase: False\n",
        "    max_sent_length: 100\n",
        "    src_vocab: \"data/{name}/vocab.txt\"\n",
        "    trg_vocab: \"data/{name}/vocab.txt\"\n",
        "\n",
        "testing:\n",
        "    beam_size: 5\n",
        "    alpha: 1.0\n",
        "\n",
        "training:\n",
        "    #load_model: \"/content/drive/MyDrive/Colab Notebooks/en-am translation/models/330000.ckpt\" # if given, load a pre-trained model from this checkpoint\n",
        "    random_seed: 42\n",
        "    optimizer: \"adam\"\n",
        "    normalization: \"tokens\"\n",
        "    adam_betas: [0.9, 0.999] \n",
        "    scheduling: \"noam\"            # Try switching from plateau to Noam scheduling\n",
        "    learning_rate_factor: 0.5       # factor for Noam scheduler (used with Transformer)\n",
        "    learning_rate_warmup: 1000      # warmup steps for Noam scheduler (used with Transformer)\n",
        "    patience: 8\n",
        "    decrease_factor: 0.7\n",
        "    loss: \"crossentropy\"\n",
        "    learning_rate: 0.0002\n",
        "    learning_rate_min: 0.00000001\n",
        "    weight_decay: 0.0\n",
        "    label_smoothing: 0.1\n",
        "    batch_size: 4096\n",
        "    batch_type: \"token\"\n",
        "    eval_batch_size: 3600\n",
        "    eval_batch_type: \"token\"\n",
        "    batch_multiplier: 1\n",
        "    early_stopping_metric: \"ppl\"\n",
        "    epochs: 2 # TODO: Decrease for when playing around and checking of working. Around 30 is sufficient to check if its working at all\n",
        "    validation_freq: 400 # Decrease this for testing\n",
        "    logging_freq: 100\n",
        "    eval_metric: \"bleu\"\n",
        "    model_dir: \"models/{name}_transformer\"\n",
        "    overwrite: True\n",
        "    shuffle: True\n",
        "    use_cuda: True\n",
        "    max_output_length: 100\n",
        "    print_valid_sents: [0, 1, 2, 3]\n",
        "    keep_last_ckpts: 3\n",
        "\n",
        "model:\n",
        "    initializer: \"xavier\"\n",
        "    bias_initializer: \"zeros\"\n",
        "    init_gain: 1.0\n",
        "    embed_initializer: \"xavier\"\n",
        "    embed_init_gain: 1.0\n",
        "    tied_embeddings: True\n",
        "    tied_softmax: True\n",
        "    encoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "    decoder:\n",
        "        type: \"transformer\"\n",
        "        num_layers: 6\n",
        "        num_heads: 8\n",
        "        embeddings:\n",
        "            embedding_dim: 512\n",
        "            scale: True\n",
        "            dropout: 0.\n",
        "        # typically ff_size = 4 x hidden_size\n",
        "        hidden_size: 512\n",
        "        ff_size: 2048\n",
        "        dropout: 0.3\n",
        "\"\"\".format(name=name, source_language=source_language, target_language=target_language)\n",
        "with open(\"joeynmt/configs/transformer_{name}.yaml\".format(name=name),'w') as f:\n",
        "    f.write(config)"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkGWuaMniT0h"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/en-am translation/models/330000.ckpt\" \"/content/joeynmt/configs/models\""
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ywc9FCweHCvz",
        "outputId": "144a0cf1-c3e5-4231-fd8e-2db4b2f4a511"
      },
      "source": [
        "! pip install torch==1.8.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.8.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.8.0+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.0+cu101) (1.20.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkxjU17f0MGf",
        "outputId": "541ac435-1d97-43d4-ca7e-19a8d94482d8"
      },
      "source": [
        "!cd joeynmt; python3 -m joeynmt train /content/joeynmt/configs/transformer_$src$tgt.yaml"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-14 11:56:46,086 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-04-14 11:56:46,119 - INFO - joeynmt.data - Loading training data...\n",
            "2021-04-14 11:56:46,663 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-04-14 11:56:46,990 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-04-14 11:56:47,011 - INFO - joeynmt.data - Loading test data...\n",
            "2021-04-14 11:56:47,026 - INFO - joeynmt.data - Data loaded.\n",
            "2021-04-14 11:56:47,026 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-04-14 11:56:47,703 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-04-14 11:56:47.823033: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-04-14 11:56:49,199 - INFO - joeynmt.training - Total params: 46475264\n",
            "2021-04-14 11:56:52,522 - INFO - joeynmt.helpers - cfg.name                           : enam_transformer\n",
            "2021-04-14 11:56:52,522 - INFO - joeynmt.helpers - cfg.data.src                       : en\n",
            "2021-04-14 11:56:52,522 - INFO - joeynmt.helpers - cfg.data.trg                       : am\n",
            "2021-04-14 11:56:52,522 - INFO - joeynmt.helpers - cfg.data.train                     : data/enam/train.bpe\n",
            "2021-04-14 11:56:52,522 - INFO - joeynmt.helpers - cfg.data.dev                       : data/enam/dev.bpe\n",
            "2021-04-14 11:56:52,522 - INFO - joeynmt.helpers - cfg.data.test                      : data/enam/test.bpe\n",
            "2021-04-14 11:56:52,522 - INFO - joeynmt.helpers - cfg.data.level                     : bpe\n",
            "2021-04-14 11:56:52,522 - INFO - joeynmt.helpers - cfg.data.lowercase                 : False\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.data.max_sent_length           : 100\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.data.src_vocab                 : data/enam/vocab.txt\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.data.trg_vocab                 : data/enam/vocab.txt\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.testing.beam_size              : 5\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.testing.alpha                  : 1.0\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.training.random_seed           : 42\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.training.optimizer             : adam\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.training.normalization         : tokens\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.training.adam_betas            : [0.9, 0.999]\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.training.scheduling            : noam\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.training.learning_rate_factor  : 0.5\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.training.learning_rate_warmup  : 1000\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.training.patience              : 8\n",
            "2021-04-14 11:56:52,523 - INFO - joeynmt.helpers - cfg.training.decrease_factor       : 0.7\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.loss                  : crossentropy\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.learning_rate         : 0.0002\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.learning_rate_min     : 1e-08\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.weight_decay          : 0.0\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.label_smoothing       : 0.1\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.batch_size            : 4096\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.batch_type            : token\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.eval_batch_size       : 3600\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.eval_batch_type       : token\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.batch_multiplier      : 1\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.early_stopping_metric : ppl\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.epochs                : 2\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.validation_freq       : 400\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.logging_freq          : 100\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.eval_metric           : bleu\n",
            "2021-04-14 11:56:52,524 - INFO - joeynmt.helpers - cfg.training.model_dir             : models/enam_transformer\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.training.overwrite             : True\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.training.shuffle               : True\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.training.use_cuda              : True\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.training.max_output_length     : 100\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.training.print_valid_sents     : [0, 1, 2, 3]\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.training.keep_last_ckpts       : 3\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.model.initializer              : xavier\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.model.bias_initializer         : zeros\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.model.init_gain                : 1.0\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.model.embed_initializer        : xavier\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.model.embed_init_gain          : 1.0\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.model.tied_embeddings          : True\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.model.tied_softmax             : True\n",
            "2021-04-14 11:56:52,525 - INFO - joeynmt.helpers - cfg.model.encoder.type             : transformer\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.encoder.num_layers       : 6\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.encoder.num_heads        : 8\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.embedding_dim : 512\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.scale : True\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.encoder.embeddings.dropout : 0.0\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.encoder.hidden_size      : 512\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.encoder.ff_size          : 2048\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.encoder.dropout          : 0.3\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.decoder.type             : transformer\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.decoder.num_layers       : 6\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.decoder.num_heads        : 8\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.embedding_dim : 512\n",
            "2021-04-14 11:56:52,526 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.scale : True\n",
            "2021-04-14 11:56:52,527 - INFO - joeynmt.helpers - cfg.model.decoder.embeddings.dropout : 0.0\n",
            "2021-04-14 11:56:52,527 - INFO - joeynmt.helpers - cfg.model.decoder.hidden_size      : 512\n",
            "2021-04-14 11:56:52,527 - INFO - joeynmt.helpers - cfg.model.decoder.ff_size          : 2048\n",
            "2021-04-14 11:56:52,527 - INFO - joeynmt.helpers - cfg.model.decoder.dropout          : 0.3\n",
            "2021-04-14 11:56:52,527 - INFO - joeynmt.helpers - Data set sizes: \n",
            "\ttrain 29017,\n",
            "\tvalid 1001,\n",
            "\ttest 1001\n",
            "2021-04-14 11:56:52,527 - INFO - joeynmt.helpers - First training example:\n",
            "\t[SRC] s@@ our@@ ce@@ _@@ sen@@ ten@@ ce\n",
            "\t[TRG] tar@@ ge@@ t@@ _@@ sen@@ ten@@ ce\n",
            "2021-04-14 11:56:52,527 - INFO - joeynmt.helpers - First 10 words (src): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) of (7) to (8) በ@@ (9) የ@@\n",
            "2021-04-14 11:56:52,527 - INFO - joeynmt.helpers - First 10 words (trg): (0) <unk> (1) <pad> (2) <s> (3) </s> (4) the (5) and (6) of (7) to (8) በ@@ (9) የ@@\n",
            "2021-04-14 11:56:52,527 - INFO - joeynmt.helpers - Number of Src words (types): 4560\n",
            "2021-04-14 11:56:52,528 - INFO - joeynmt.helpers - Number of Trg words (types): 4560\n",
            "2021-04-14 11:56:52,528 - INFO - joeynmt.training - Model(\n",
            "\tencoder=TransformerEncoder(num_layers=6, num_heads=8),\n",
            "\tdecoder=TransformerDecoder(num_layers=6, num_heads=8),\n",
            "\tsrc_embed=Embeddings(embedding_dim=512, vocab_size=4560),\n",
            "\ttrg_embed=Embeddings(embedding_dim=512, vocab_size=4560))\n",
            "2021-04-14 11:56:52,532 - INFO - joeynmt.training - Train stats:\n",
            "\tdevice: cuda\n",
            "\tn_gpu: 1\n",
            "\t16-bits training: False\n",
            "\tgradient accumulation: 1\n",
            "\tbatch size per device: 4096\n",
            "\ttotal batch size (w. parallel & accumulation): 4096\n",
            "2021-04-14 11:56:52,532 - INFO - joeynmt.training - EPOCH 1\n",
            "2021-04-14 11:57:27,163 - INFO - joeynmt.training - Epoch   1, Step:      100, Batch Loss:     6.240597, Tokens per Sec:     8101, Lr: 0.000070\n",
            "2021-04-14 11:58:02,505 - INFO - joeynmt.training - Epoch   1, Step:      200, Batch Loss:     6.020729, Tokens per Sec:     7934, Lr: 0.000140\n",
            "2021-04-14 11:58:38,830 - INFO - joeynmt.training - Epoch   1, Step:      300, Batch Loss:     5.917579, Tokens per Sec:     7826, Lr: 0.000210\n",
            "2021-04-14 11:59:06,405 - INFO - joeynmt.training - Epoch   1: total training loss 2306.44\n",
            "2021-04-14 11:59:06,405 - INFO - joeynmt.training - EPOCH 2\n",
            "2021-04-14 11:59:15,425 - INFO - joeynmt.training - Epoch   2, Step:      400, Batch Loss:     5.267385, Tokens per Sec:     7411, Lr: 0.000280\n",
            "2021-04-14 12:01:14,852 - INFO - joeynmt.training - Hooray! New best validation result [ppl]!\n",
            "2021-04-14 12:01:14,852 - INFO - joeynmt.training - Saving new checkpoint.\n",
            "2021-04-14 12:01:16,565 - INFO - joeynmt.training - Example #0\n",
            "2021-04-14 12:01:16,565 - INFO - joeynmt.training - \tSource:     source_sentence\n",
            "2021-04-14 12:01:16,565 - INFO - joeynmt.training - \tReference:  target_sentence\n",
            "2021-04-14 12:01:16,565 - INFO - joeynmt.training - \tHypothesis: 3 የየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ን ን ን ን ን ይን ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ\n",
            "2021-04-14 12:01:16,565 - INFO - joeynmt.training - Example #1\n",
            "2021-04-14 12:01:16,566 - INFO - joeynmt.training - \tSource:     \"14 and i was making greater progress in juʹda·ism than many of my own age in my nation, as i was far more zealous for the traditions of my fathers.+\"\n",
            "2021-04-14 12:01:16,566 - INFO - joeynmt.training - \tReference:  14 ለአባቶቼ ወግ ከፍተኛ ቅንዓት ስለነበረኝ ከወገኖቼ መካከል በእኔ ዕድሜ ካሉት ከብዙዎቹ የበለጠ በአይሁዳውያን ሃይማኖት የላቀ እድገት እያደረግኩ ነበር።+\n",
            "2021-04-14 12:01:16,566 - INFO - joeynmt.training - \tHypothesis: 9 “““““““““የየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ን ን ን ን ን ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ\n",
            "2021-04-14 12:01:16,566 - INFO - joeynmt.training - Example #2\n",
            "2021-04-14 12:01:16,566 - INFO - joeynmt.training - \tSource:     \"15 but when god, who separated me from my mother’s womb and called me through his undeserved kindness,+ thought good\"\n",
            "2021-04-14 12:01:16,566 - INFO - joeynmt.training - \tReference:  15 ሆኖም ከእናቴ ማህፀን እንድለይ* ያደረገኝና በጸጋው አማካኝነት የጠራኝ አምላክ+\n",
            "2021-04-14 12:01:16,566 - INFO - joeynmt.training - \tHypothesis: 9 ““““““““““““የየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየበበበበበበበበበበበበበበበበበበበበበበበን ን ን ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ ላይ\n",
            "2021-04-14 12:01:16,566 - INFO - joeynmt.training - Example #3\n",
            "2021-04-14 12:01:16,566 - INFO - joeynmt.training - \tSource:     \"16 to reveal his son through me so that i might declare the good news about him to the nations,+ i did not immediately consult with any human;*\"\n",
            "2021-04-14 12:01:16,567 - INFO - joeynmt.training - \tReference:  16 ስለ ክርስቶስ የሚገልጸውን ምሥራች ለአሕዛብ እንዳውጅ+ ልጁን በእኔ አማካኝነት ለመግለጥ በወደደ ጊዜ ከማንም ሰው* ጋር ወዲያው አልተማከርኩም፤\n",
            "2021-04-14 12:01:16,567 - INFO - joeynmt.training - \tHypothesis: 9 ““““““““““““““““““““““የየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየየበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበበራ።\n",
            "2021-04-14 12:01:16,567 - INFO - joeynmt.training - Validation result (greedy) at epoch   2, step      400: bleu:   0.00, loss: 201899.6250, ppl: 252.6364, duration: 121.1414s\n",
            "2021-04-14 12:01:56,583 - INFO - joeynmt.training - Epoch   2, Step:      500, Batch Loss:     5.329358, Tokens per Sec:     7100, Lr: 0.000349\n",
            "2021-04-14 12:02:35,967 - INFO - joeynmt.training - Epoch   2, Step:      600, Batch Loss:     5.313131, Tokens per Sec:     7057, Lr: 0.000419\n",
            "2021-04-14 12:03:15,873 - INFO - joeynmt.training - Epoch   2, Step:      700, Batch Loss:     4.925463, Tokens per Sec:     7083, Lr: 0.000489\n",
            "2021-04-14 12:03:36,149 - INFO - joeynmt.training - Epoch   2: total training loss 1984.01\n",
            "2021-04-14 12:03:36,149 - INFO - joeynmt.training - Training ended after   2 epochs.\n",
            "2021-04-14 12:03:36,149 - INFO - joeynmt.training - Best validation result (greedy) at step      400: 252.64 ppl.\n",
            "2021-04-14 12:03:36,165 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
            "2021-04-14 12:03:36,562 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-04-14 12:03:37,200 - INFO - joeynmt.model - Enc-dec model built.\n",
            "2021-04-14 12:03:37,361 - INFO - joeynmt.prediction - Decoding on dev set (data/enam/dev.bpe.am)...\n",
            "2021-04-14 12:12:34,699 - INFO - joeynmt.prediction -  dev bleu[13a]:   0.01 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-04-14 12:12:34,700 - INFO - joeynmt.prediction - Translations saved to: models/enam_transformer/00000400.hyps.dev\n",
            "2021-04-14 12:12:34,700 - INFO - joeynmt.prediction - Decoding on test set (data/enam/test.bpe.am)...\n",
            "2021-04-14 12:21:43,678 - INFO - joeynmt.prediction - test bleu[13a]:   0.01 [Beam search decoding with beam size = 5 and alpha = 1.0]\n",
            "2021-04-14 12:21:43,680 - INFO - joeynmt.prediction - Translations saved to: models/enam_transformer/00000400.hyps.test\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wSyOBpksEOv",
        "outputId": "05918ca7-f72e-4095-dd0d-99717dcee2cb"
      },
      "source": [
        "! cat joeynmt/models/enam_transformer/validations.txt"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steps: 400\tLoss: 201899.62500\tPPL: 252.63644\tbleu: 0.00312\tLR: 0.00027951\t*\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZJGEqJdnPsh",
        "outputId": "e0272a88-5877-4f68-c92e-eb59a78917d0"
      },
      "source": [
        "# Copy the created models from the notebook storage to google drive for persistant storage \n",
        "!mkdir \"$gdrive_path/models/\"\n",
        "!cp -r joeynmt/models/* \"$gdrive_path/models/${src}${tgt}_transformer/\""
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/Colab Notebooks/en-am translation/en-am/models/’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDaExTv7KhWD"
      },
      "source": [
        "! cat \"$gdrive_path/models/${src}${tgt}_transformer/validations.txt\""
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrehNFeVwNzZ"
      },
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/en-am translation/models/330000.ckpt\" \"/content/joeynmt/models/enam_transformer\""
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWF0hZsmjpFc",
        "outputId": "c4ceec21-25a8-4761-d699-d7b492c88fbb"
      },
      "source": [
        "! cd joeynmt; python3 -m joeynmt test models/enam_transformer/config.yaml"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-04-14 12:34:54,195 - INFO - root - Hello! This is Joey-NMT (version 1.3).\n",
            "2021-04-14 12:34:54,196 - INFO - joeynmt.data - Building vocabulary...\n",
            "2021-04-14 12:34:54,533 - INFO - joeynmt.data - Loading dev data...\n",
            "2021-04-14 12:34:54,546 - INFO - joeynmt.data - Loading test data...\n",
            "2021-04-14 12:34:54,560 - INFO - joeynmt.data - Data loaded.\n",
            "2021-04-14 12:34:54,601 - INFO - joeynmt.prediction - Process device: cuda, n_gpu: 1, batch_size per device: 18000 (with beam_size)\n",
            "2021-04-14 12:34:58,180 - INFO - joeynmt.model - Building an encoder-decoder model...\n",
            "2021-04-14 12:34:58,895 - INFO - joeynmt.model - Enc-dec model built.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 48, in <module>\n",
            "    main()\n",
            "  File \"/content/joeynmt/joeynmt/__main__.py\", line 38, in main\n",
            "    output_path=args.output_path, save_attention=args.save_attention)\n",
            "  File \"/content/joeynmt/joeynmt/prediction.py\", line 311, in test\n",
            "    model.load_state_dict(model_checkpoint[\"model_state\"])\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\", line 1224, in load_state_dict\n",
            "    self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
            "RuntimeError: Error(s) in loading state_dict for Model:\n",
            "\tsize mismatch for src_embed.lut.weight: copying a param with shape torch.Size([10868, 256]) from checkpoint, the shape in current model is torch.Size([4560, 512]).\n",
            "\tsize mismatch for trg_embed.lut.weight: copying a param with shape torch.Size([10868, 256]) from checkpoint, the shape in current model is torch.Size([4560, 512]).\n",
            "\tsize mismatch for encoder.layers.0.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.0.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.0.src_src_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.0.src_src_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.0.src_src_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.0.src_src_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.0.src_src_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.0.src_src_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.0.src_src_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.0.src_src_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.0.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.0.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.0.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for encoder.layers.0.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for encoder.layers.0.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for encoder.layers.0.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.1.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.1.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.1.src_src_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.1.src_src_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.1.src_src_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.1.src_src_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.1.src_src_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.1.src_src_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.1.src_src_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.1.src_src_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.1.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.1.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.1.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for encoder.layers.1.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for encoder.layers.1.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for encoder.layers.1.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.2.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.2.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.2.src_src_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.2.src_src_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.2.src_src_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.2.src_src_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.2.src_src_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.2.src_src_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.2.src_src_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.2.src_src_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.2.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.2.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.2.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for encoder.layers.2.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for encoder.layers.2.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for encoder.layers.2.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.3.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.3.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.3.src_src_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.3.src_src_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.3.src_src_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.3.src_src_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.3.src_src_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.3.src_src_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.3.src_src_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.3.src_src_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.3.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.3.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.3.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for encoder.layers.3.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for encoder.layers.3.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for encoder.layers.3.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.4.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.4.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.4.src_src_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.4.src_src_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.4.src_src_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.4.src_src_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.4.src_src_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.4.src_src_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.4.src_src_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.4.src_src_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.4.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.4.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.4.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for encoder.layers.4.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for encoder.layers.4.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for encoder.layers.4.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.5.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.5.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.5.src_src_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.5.src_src_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.5.src_src_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.5.src_src_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.5.src_src_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.5.src_src_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.5.src_src_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for encoder.layers.5.src_src_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.5.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.5.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layers.5.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for encoder.layers.5.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for encoder.layers.5.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for encoder.layers.5.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for encoder.pe.pe: copying a param with shape torch.Size([1, 5000, 256]) from checkpoint, the shape in current model is torch.Size([1, 5000, 512]).\n",
            "\tsize mismatch for decoder.layers.0.trg_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.0.trg_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.trg_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.0.trg_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.trg_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.0.trg_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.trg_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.0.trg_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.src_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.0.src_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.src_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.0.src_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.src_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.0.src_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.src_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.0.src_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for decoder.layers.0.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for decoder.layers.0.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for decoder.layers.0.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.x_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.x_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.dec_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.0.dec_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.trg_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.1.trg_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.trg_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.1.trg_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.trg_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.1.trg_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.trg_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.1.trg_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.src_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.1.src_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.src_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.1.src_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.src_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.1.src_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.src_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.1.src_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for decoder.layers.1.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for decoder.layers.1.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for decoder.layers.1.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.x_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.x_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.dec_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.1.dec_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.trg_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.2.trg_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.trg_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.2.trg_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.trg_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.2.trg_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.trg_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.2.trg_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.src_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.2.src_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.src_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.2.src_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.src_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.2.src_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.src_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.2.src_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for decoder.layers.2.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for decoder.layers.2.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for decoder.layers.2.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.x_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.x_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.dec_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.2.dec_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.trg_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.3.trg_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.trg_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.3.trg_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.trg_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.3.trg_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.trg_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.3.trg_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.src_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.3.src_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.src_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.3.src_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.src_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.3.src_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.src_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.3.src_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for decoder.layers.3.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for decoder.layers.3.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for decoder.layers.3.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.x_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.x_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.dec_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.3.dec_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.trg_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.4.trg_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.trg_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.4.trg_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.trg_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.4.trg_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.trg_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.4.trg_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.src_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.4.src_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.src_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.4.src_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.src_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.4.src_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.src_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.4.src_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for decoder.layers.4.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for decoder.layers.4.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for decoder.layers.4.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.x_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.x_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.dec_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.4.dec_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.trg_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.5.trg_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.trg_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.5.trg_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.trg_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.5.trg_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.trg_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.5.trg_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.src_trg_att.k_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.5.src_trg_att.k_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.src_trg_att.v_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.5.src_trg_att.v_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.src_trg_att.q_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.5.src_trg_att.q_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.src_trg_att.output_layer.weight: copying a param with shape torch.Size([256, 256]) from checkpoint, the shape in current model is torch.Size([512, 512]).\n",
            "\tsize mismatch for decoder.layers.5.src_trg_att.output_layer.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.feed_forward.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.feed_forward.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.feed_forward.pwff_layer.0.weight: copying a param with shape torch.Size([1024, 256]) from checkpoint, the shape in current model is torch.Size([2048, 512]).\n",
            "\tsize mismatch for decoder.layers.5.feed_forward.pwff_layer.0.bias: copying a param with shape torch.Size([1024]) from checkpoint, the shape in current model is torch.Size([2048]).\n",
            "\tsize mismatch for decoder.layers.5.feed_forward.pwff_layer.3.weight: copying a param with shape torch.Size([256, 1024]) from checkpoint, the shape in current model is torch.Size([512, 2048]).\n",
            "\tsize mismatch for decoder.layers.5.feed_forward.pwff_layer.3.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.x_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.x_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.dec_layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layers.5.dec_layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.pe.pe: copying a param with shape torch.Size([1, 5000, 256]) from checkpoint, the shape in current model is torch.Size([1, 5000, 512]).\n",
            "\tsize mismatch for decoder.layer_norm.weight: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.layer_norm.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([512]).\n",
            "\tsize mismatch for decoder.output_layer.weight: copying a param with shape torch.Size([10868, 256]) from checkpoint, the shape in current model is torch.Size([4560, 512]).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvOihUEwfmik",
        "outputId": "0f8e9dc6-5012-40ca-bef7-a71387bdce97"
      },
      "source": [
        "cd joeynmt; python3 -m joeynmt translate models/enam_transformer/config.yaml < en.txt > enam_translate_output"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'joeynmt; python3 -m joeynmt translate models/enam_transformer/config.yaml < en.txt > enam_translate_output'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBbbpNiyKyTL",
        "outputId": "1fd92bac-d2b6-4ef6-bb97-381eeeacdcab"
      },
      "source": [
        "! cd joeynmt; python3 -m joeynmt translate /content/joeynmt/models/enam_transformer/ < en.txt > enam_translate_output"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: en.txt: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmVWGKIj_wT4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "1f40e10a-c7ea-4bbd-a6e2-8c8f060ad5a2"
      },
      "source": [
        "from subword_nmt import apply_bpe\n",
        "\n",
        "with open(bpe_file, \"r\") as merge_file:\n",
        "  bpe = apply_bpe.BPE(codes=merge_file)\n",
        "\n",
        "preprocess = lambda x: bpe.process_line(x.strip())"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-25d5806e47cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msubword_nmt\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_bpe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbpe_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmerge_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0mbpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_bpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerge_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'bpe_file' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t08bAifocgz2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}